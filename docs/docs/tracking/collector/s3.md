---
sidebar_position: 4
title: Amazon S3
slug: /tracking/collector/amazon-s3
---

# Amazon S3

The [Objectiv Collector](./introduction.md) can be configured to store data on Amazon S3 through 
[Snowplow](./snowplow-pipeline.md) on AWS. The Snowplow AWS setup uses either SQS (message queue) or Kinesis 
to connect various stages in the pipeline. 

:::info
Objectiv's [data modeling libraries](../../modeling/index.mdx) do not yet support querying data stored on S3.
It's currently mainly useful for additional archiving next to the supported data stores.
:::

## How to set up Objectiv on AWS through Snowplow

:::note
We assume below that you've already read [how to set up Objectiv with Snowplow](./snowplow-pipeline.md).
:::

The setup works as follows:
- Events arrive at the Objectiv Collector, and are validated;
- Good events are published on the `raw` topic on Kinesis or SQS (which in turn is processed by Enrich); and
- Bad events (invalid) are published on the `bad` topic on Kinesis.

### Starting the Collector
The output topics of the Collector are controlled through environment variables:

- `SP_AWS_MESSAGE_TOPIC_RAW` - this can be either the id of a Kinesis stream (eg. sp-raw-stream) _or_ a URL 
  to an SQS queue (eg. https://sqs.someregion.amazonaws.com/123455/sp-raw-queue);
- `SP_AWS_MESSAGE_TOPIC_BAD` - this should be the id of the Kinesis bad stream (eg. sp-bad-stream).

The AWS integration uses the boto3 python library, which means authentication is also provided through that 
library (as detailed [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)). 
The simplest way to make it work, is by setting the following environment variables:

- `AWS_ACCESS_KEY_ID` - iAM key of the account used to access AWS services;
- `AWS_SECRET_ACCESS_KEY` - iAM secret key;
- `AWS_DEFAULT_REGION` - Optionally specify the AWS region in which the Kinesis/SQS resources are deployed.

Once these environment variables have been set, the Objectiv Collector can be started, and the Snowplow AWS 
output will be enabled.

### Using docker-compose
To run this setup in docker, make sure that the aforementioned environment variables are properly set and 
available in the container. Also take care that the path to the credentials is actually available in the 
container.

When using `docker-compose`, the following yaml snippet will do the trick:
```yaml
objectiv_collector:
  container_name: objectiv_collector
  image: objectiv/backend
  working_dir: /services
  ports:
    - "127.0.0.1:5000:5000"
  environment:
    AWS_ACCESS_KEY_ID: AKIA-some-key
    AWS_SECRET_ACCESS_KEY: some-secret-key
    AWS_DEFAULT_REGION: some-aws-region
    SP_AWS_MESSAGE_TOPIC_RAW: sp-raw-stream
    SP_AWS_MESSAGE_TOPIC_BAD: sp-bad-stream
    OUTPUT_ENABLE_PG: false
```

The important part here is setting the correct env:
- providing the AWS credentials;
- providing the Kinesis stream ids.

### Running locally
Running the Collector locally, in a dev setup is pretty similar:

```sh
# setup environment
virtualenv objectiv-venv
source objectiv-venv/bin/activate
pip install -r requirements.in

# start flask app, using SQS queue
cd objectiv_backend
export PYTHONPATH=.:$PYTHONPATH
SP_AWS_MESSAGE_TOPIC_RAW=sp-raw-stream \
  SP_AWS_MESSAGE_TOPIC_BAD=sp-bad-stream \
  flask run
```

### Test the setup
The Collector will display a message if the Snowplow config is loaded: `Enabled Snowplow: AWS pipeline 
($SP_AWS_MESSAGE_TOPIC)`.

This indicates that the Collector will try to push events. If this fails, logging should hint what's 
happening. If there are no errors in the Collector logs, the events should be successfully pushed into the 
raw topic, to be picked up by Snowplow's enrichment.

To check if the messages have successfully arrived in the queue, please review to monitoring in the AWS 
console. Events should show up as either `PutRecords` (Kinesis) or `Number of messages received (SQS).
